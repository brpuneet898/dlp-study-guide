---
title: "Introduction"
format:
  html:
    code-fold: false
jupyter: python3
page-navigation: true
---

# Introduction

## Evolution of Hugging Face

Hugging Face has proven to be an indispensable tool in the artificial intelligence (AI) domain. It has paved a long way, evolving from a niche-specific chatbot startup into a $4.5 billion open-source powerhouse and delivering access to machine learning, deep learning, and other associated domains. It was founded sometime around 2016. Initially, the company started as a chatbot app that used to target teenagers to provide emotional support via an AI BFF (Best Friend Forever). Soon enough, the founders realized that it had a lot of potential than just being used as a chatbot. They found the underlying principle of natural language processing (NLP) technology. Finally, open-sourced in 2018, they started building a collaborative environment for AI/ML development. This decision led them to become the major leader in the ML ecosystem.
The Transformers library was released by Hugging Face (hereafter referred to as HF) in 2018. It was a toolkit based on Python Language that provided simplified access to pre-trained models like BERT and GPT.

```{python}
## Use this to download the Transformers library into your environment.

## Uncomment the Code (line starting with a single #) and run it in your choice of environment. 

# !pip install transformers
``` 

This was huge. It implemented state-of-the-art NLP without having to procure all the extensive computational resources. This modular design was accepted widely and the documentation flourished. By 2020, 1000 open-source models were available. The open-source feature allowed the global community to participate as contributors. Thus, innovation accelerated with no bounds. Slowly, all kinds of NLP tasks like text classification, sentiment analysis, multilingual translation, etc. HF had models and datasets for all of them. Slowly in the 2020s, the platform decided to go beyond NLP. The related domains like Reinforcement Learning, and computer vision were introduced. At the same time, enterprise-level features were introduced, that included Autotraining, private cloud hosting, and better and informed security protocols. The demand for HF started growing and Fortune 500 companies like Intel, and Bloomberg, started aligning HF in commercialization.

In 2022, HF launched BLOOM, a multilingual large language model with 176 billion parameters, this made the community hit with a critical mass. BLOOM supported 46 languages and 13 programming languages which almost challenged the existing models like GPT-3 (trained on 175 billion parameters). The platform surpassed 1 million AI models in October 2024. The tools enhanced for image generation (Stable Diffusion), writing code (CodeGen), and audio processing. This milestone led to, HF being referred to as "GitHub of Machine Learning."
The major strategic partnership came to HF from AWS integrated into HF tools via Trainium chips. Soon, other collaborations from Meta and UNESCO launched a translator that supported 200 languages, even with some specific dialects. Investors from Google, NVIDIA, AMD, etc recognized the potential and the company achieved a $4.5 billion valuation.

A few of the recent expansions show the HF's ambition to lead the AI workspace -

- European AI Accelerator was launched in June 2024 with Meta and Scaleway. This program acts as a mentor for the startups in Paris to integrate open models to reach a stage of commercial products.
- AI Ethics - Allowing BLOOM's open weights helped to establish a standard model card and transparency in the dataset. It helped to promote reproducibility and advocated ethical AI.
- Acquisitions - In 2022, HF acquired Gradio, another powerful, open-source ML App builder. It helped many developers to streamline deployment pipelines.

*Delangue* once stated, *"Smaller, yet specialized models often outperform generic ones."* According to the philosophy, HF is leading the coming wave of artificial intelligence. It helps to make the future more helpful, and adaptable, empowering diverse applications using open tools and applicable everywhere ranging from healthcare diagnostics to climate modeling. Thus, I hope the incoming students of Deep Learning Practice, can understand the exemplification of HF by following the open-source principles to scale innovation at a global level, doing all, by maintaining ethical accountability. 

## Usage of Hugging Face and Best Practices

As discussed, Hugging Face is called the *GitHub of Machine Learning*. It offers around 300,000+ pre-trained models, 50,000+ datasets, number of tools to build or deploy your AI applications. These customizable models and datasets can be trained and tested for various domains. There are several open-source libraries like `transformers` (specifically designed for Natural Language Processing - this will be highly used in our course work from Week 1 to 4), `diffusers` (designed for image generation), `datasets` (download datasets directly by specifying the URL and intended sub-parts). They collectively help us in working with modern AI architectures like BERT, GPT, and stable diffusion. 

**Key Components -**

- **Models:** Pre-trained architectures like BERT, GPT, and Diffusion for NLP, vision, and audio tasks. Ready to download and use. Please note some of the models are `gated`, they require special permissions from the owners. For eg. `meta-llama/Llama-3.2-1B`
- **Datasets:** Curated and clean data ready for training or evaluation. We will learn in the next module how to download and play with the datasets. 
- **Spaces:** Host ML demos using Gradio or Streamlit. Not very relevant for this particular course, but highly recommended for real-life industrial applications. 
- **Inference API:** Serverless and seamless experience in model testing. 

**Setup for Deployment**

**1. VS Code**

- Install [Hugging Face VS Code Extension](https://marketplace.visualstudio.com/items?itemName=HuggingFace.huggingface-vscode)
- Configure API token:
```{bash}
## In VS Code command palette (Ctrl+Shift+P):
# Hugging Face Code: Set API Token
```

- Use code autocompletion with StarCoder/Codellama

**2. Google Colab**

```{python}
## Install core libraries
# !pip install transformers datasets huggingface_hub

## Authenticate
# from huggingface_hub import notebook_login
# notebook_login()  # Follow token prompt
```

**3. CLI in Token Management**

- Create token at [hf.co/settings/tokens](https://huggingface.co/settings/tokens)
- Terminal login:
```{bash}
# huggingface-cli login
## Enter token when prompted
```

- Use in Python:
```{python}
# from huggingface_hub import login
# login(token="hf_yourtoken")  # For script authentication
```

### Best Practices

1. Model Selection 
  - Filter by task/library/dataset on Model Hub
  - Check model cards for license/compute requirements
2. Resource Management
  - Use `fp16` precision: `model.half()`
  - Leverage model parallelism for large models
3. Reproducibility
```{python}
# from transformers import set_seed
# set_seed(42)  # Across all ML libraries
```

4. Production Deployment
  - Convert models to ONNX/TensorRT
  - Use Inference Endpoints for scalable serving
5. Convention used in our study guide - 
  - `#` (Single hash): Executable code (uncomment to use)
  - `##` (Double hash): Explanatory comments