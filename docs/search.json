[
  {
    "objectID": "sneak.html",
    "href": "sneak.html",
    "title": "Sneak Peak into Important Libraries",
    "section": "",
    "text": "In this section, we will discuss the important libraries that we will be using in the Deep Learning Course.\n\nHugging Face Datasets\n\nTo install -\n\n# !pip install datasets\n\nHugging Face Datasets is a powerful library provided by Hugging Face that simplifies loading and processing datasets for various machine learning and deep learning tasks. Over the years, it has become a unified interface for thousands of datasets available on the hub. It provides datasets for various domains starting from audio, computer vision, and natural language processing. The library allows the users to load the datasets with a single piece of code and offers advanced processing algorithms to prepare and customize the data for training in deep learning models. Further, it allows to implement of the various performance metrics that are needed for transformer-based model evaluations.\nKey features -\n\n\nEasy access to a vast collection of curated datasets from across the world.\n\n\nEfficient data loading and further processing capabilities.\n\n\nEasy integration with other related libraries.\n\n\nBuilt-in utilities for data preparation, splitting, shuffling, and mapping.\n\n\nFlexibility to fine-tune models on customized needs and datasets.\n\n\n\nHugging Face Transformers\n\nTo install -\n\n# !pip install transformers\n\nThe Hugging Face Transformers is another important and comprehensive library that provides state-of-the-art pre-trained models for various natural language processing tasks. It offers all types of models including - BERT, GPT, and T5, which can be loaded and fine-tuned for customizable tasks. The library supports other multiple deep learning frameworks such as PyTorch, TensorFlow, and JAX. Further, it makes it versatile for different development environments.\nKey features -\n\n\nAccess to a vast collection of pre-trained models.\n\n\nSupport most of the deep learning frameworks.\n\n\nEasy to use APIs for inference and future fine-tuning.\n\n\nExtensive community support is well maintained by tech leaders globally.\n\n\n\nPyTorch\n\nTo install -\n\n# !pip install torch\n\nPyTorch is commonly known as Torch. It is one of the most powerful open-source machine-learning libraries developed by Facebook. It is known for its dynamic computational graphs - that allows for more flexible and intuitive model development. It is quite popular among researchers in the field of deep learning due to its ease of use and powerful capabilities.\nKey features -\n\n\nDynamic computational graphs for flexible model development.\n\n\nEfficient GPU (like T4, A100, etc) acceleration for faster training and inference.\n\n\nFull collection of tools and libraries for various ML and DL tasks.\n\n\nRegular updates from the community keep everything up-to-date.\n\n\n\nPyannote\n\nTo install -\n\n# !pip install pyannote\n\nPyannote is an open-source Python toolkit. It is specifically designed for speaker diarization tasks. We will be using this library in Week 6 (the speech part). It helps to identify and cluster the fixed number of speakers from the given audio, this process in a nutshell is known as the speaker diarization. It uses the PyTorch deep learning framework. It provides trained models and pipelines, especially efficient in audio analysis. Further, it is also useful in tasks such as speaker segmentation, speaker clustering, and overlapped speech detection.\nKey features -\n\n\nProvides pre-trained models for speaker diarization tasks.\n\n\nHelps to assemble or create pipelines for audio processing.\n\n\nProvides GPU acceleration support for faster processing.\n\n\nFlexibility to fine-tune models on customized needs and datasets.\n\n\n\nWhisper\n\nTo install -\n\n# !pip install git+https://github.com/openai/whisper.git\n\n## Also, download FFmpeg for handling various audio formats with ease.\n\n# !pip install ffmpeg\n\nWhisper is another very powerful pre-trained model developed by OpenAI. We will be using this in Weeks 6 and 7. It helps in automatic speech recognition (commonly referred to as ASR) and speech translation. It has been trained on around 680,000 hours of labeled data (mostly in western languages). The vast amount of labeled data helps to generalize well across various datasets and domains, sometimes even without further fine-tuning. Whisper is available to use in different model sizes ranging from tiny to large. This helps to accommodate different computational requirements and varied use cases.\nKey features -\n\n\nRobust performance in various languages and certain accents.\n\n\nSupports speech recognition and translation tasks.\n\n\nAvailability of different model sizes making it usable for varied purposes.\n\n\nAvailable as open-source, thereby, creating a strong community.",
    "crumbs": [
      "Week 0",
      "Sneak Peak"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DLP Study Guide",
    "section": "",
    "text": "Table of Contents\n\nWeek 0\n\nIntroduction\nSneak Peak\n\nWeek 5\n\nL1: Introduction to Speech\nL2: Language Identification - Intro\nL3: Language Identification - Demo",
    "crumbs": [
      "Index"
    ]
  },
  {
    "objectID": "5_2.html",
    "href": "5_2.html",
    "title": "L2: Language Identification - Intro",
    "section": "",
    "text": "In the first week of the speech module, we are going to carry out a simple experiment of identifying the language from a given speech sample. This is a simple experiment, yet has a lot of applications in the industry. For eg. we can route customer service based on the speaker’s language and build speech recognition systems.\n\nUnderstanding the Importance of Language Identification\n\nWe can use the models for language identification to streamline customer service interactions. The process will be automatically redirected to the respective domain experts based on the speaker’s language. Thus, enabling the efficient routing of calls to the respective language-speaking agents.\nSuch models are especially very useful in vast multilingual countries like India with numerous languages. They are similarly helpful in other Western countries, where the English language has different variations and accents.\n\n\n\nOverview of the Task 1\n\nInput: Speech signal or an audio clip.\nOutput: Identified Language (eg. Spanish, Romanian, English)\nMethodology: We will solve this problem with the help of a pattern classification methodology.\n\n\n\nUnderstanding Pattern Classification\n\nLet’s understand this with the help of an example. What if we had to classify the individuals as children, women, and men based on height and weight?\nIn general, we understand that there is a sharp difference in the height-weight combination in the given 3 classes. In this, children are on the lowest bound and men on the highest edge.\nFeatures: Height, Weight\nClasses: Children, Men, and Women\nLabeled Data: We will have a known classification of labeled data for training the system.\nProcess: First of all, we find the representative vector for each of the classes. Post that, the new data point will be determined by the closest match to representative vectors for each class.\n\n\n\nLanguage Identification Process\n\nFeature Extraction - In this step, we extract the features from the group of speech signals that will help distinguish the languages. It is helpful to know the total number of distinct languages beforehand. Here, we will be using some pre-trained models like Wave2Vec 2.0 for extracting the features and speech representation.\nTraining - Once we have collected the labeled training data (we will be using a Facebook dataset for this purpose) - which is the collection of audio clips labeled with their respective languages. We can extract the features and keep them as embedding vectors from these audio clips using Wave2Vec 2.0 or any other trained model. Next, we have to use the averaging method to average out the sequence of vectors obtained, so that we have a single representation for each clip. Thus, now we can train a classification system to minimize the errors in the training data. (Use entropy loss or squared loss)\nValidation - Generally, this step is not much used, but still it is better to stay in touch with the basics. We can use a small percentage of held-out data (known as the validation set) to check how well the model has generalized. It is easier to test here since we know the correct labels. It will help the system to perform better on the unseen data.\nTesting - This step is also referred to as the Inferencing step. We can now apply our model (the trained system) to new, unlabeled audio clips. Thus, classifying the language based on the extracted features.\nImplementation - We will review the implementation for this part in the next module in more detail. We will use the Hugging Face Datasets hub to get the audio clips for three languages (namely - English, Spanish, and Romanian). For our example, we will use 50 audio clips from each of them for training purposes. Next, we have to extract the features using the Wave2Vec 2.0 model. Finally, we will be training a neural network classifier on the extracted features.",
    "crumbs": [
      "Week 5",
      "L2: Language Identification - Intro"
    ]
  },
  {
    "objectID": "5_1.html",
    "href": "5_1.html",
    "title": "L1: Introduction to Speech",
    "section": "",
    "text": "Now that we have completed the first four modules related to Natural Language Processing, we will now deal with speech technology.\n\nThe Fundamentals of Speech -\n\nSpeech, when recorded or available to us, is usually in the form of a waveform, also we can represent it in the form of a continuous-time signal.\nThe primary goal of the speech is communication.\nGoing by history, it has been recorded that speech evolved before text as a means of human communication. Even today, in the remotest of villages, even if the people can’t read or write, they can still speak (it could be their native language).\n\n\n\nWhat are the differences between speech and text?\n\nSpeech is much more difficult to visualize than processing the text.\nThe speech provides a lot of variations in understanding a particular context, just by varying the pitch and the length of the same sentence. This is more or less not possible in the text.\nThe building blocks of the text are discrete alphabets (A - Z). On the other hand, the building blocks of speech are phonemes (that use continuous signals).\n\n\n\nThe Process of Speech Production\n\nThe message that we want to convey is first of all formed in the brain.\nThere are innumerable ways to represent the same message. Eg. “Do you want to go for lunch?”, “Lunch?”, etc.\nThe variation in the pitch can convert any question to a statement or vice-versa.\nDuring the perception part, the ear processes the incoming speech signal.\nFinally, the receiver’s brain interprets the processing speech signal, thus, extracting the message.\n\n\n\nUnderstanding Phonemes\n\nAs stated, the phonemes are the basic units of speech, that usually fulfill the analogy to alphabets in the text processing.\nGenerally, each language has its own set of phonemes. Usually, a language has 45 - 60 phonemes.\nEg. Good -&gt; { /g/, /uh/, /d/ }\nA particular word identified in a speech is nothing but a sequence of phonemes. This is similar to how the continuous set of alphabets forms the textual words.\n\n\n\nDifferentiating Speech from other Audio Samples (like Music)\n\nSpeech usually has a simple and distinct structure that can be modeled using simple mathematical models. On the other hand, music or any other general audio sample is more complex and quite difficult to model.\nAudio samples like Music have harmonics that are smooth and are in rhythm. Therefore, they can not be represented in the form of a sparse matrix.\n\n\n\nUseful Features of Speech\n\nIt provides the different kinds of characteristics of a speaker. For eg. gender identification (female or male depending on the pitch length), age estimation (easy to differentiate into 3 clusters - children, adults, and old age), and useful as a voice in biometrics.\nIt conveys different types of emotional and contextual information like excitement, insistence, etc. Further, it provides a lot of context (if heard correctly) that may not be inherent clearly with the help of text.\nAs few words are spoken, it is quite easy to identify the language spoken. It has a lot of applications in support and voice call teams.\nDuring the COVID times, speech was also used to identify and reveal some respiratory or nasal issues.",
    "crumbs": [
      "Week 5",
      "L1: Introduction to Speech"
    ]
  },
  {
    "objectID": "5_3.html",
    "href": "5_3.html",
    "title": "L3: Language Identification - Demo",
    "section": "",
    "text": "In this module, we will focusing on the implementation part of the Language Identification task. The learners are requested to copy the code into their colab notebook and run the cells to see the output. Note : After copying use Ctrl + ? to uncomment the code.\n\nInstalling the Required Libraries\nFor the required task, we will be using datasets (to download the datasets), transformers, and torch library (to build complex neural networks).\n\n# !pip install transformers datasets torch\n\n\n\nData Loading\nWe will be using Voxpopuli dataset given by Facebook as our Language Identification dataset.\n\n# from datasets import load_dataset\n\n## the Id of English language is 0. We will be streaming the input so as to reduce the comptutational task of downloading the entire dataset which is too large.\n# dataset = load_dataset(\"facebook/voxpopuli\", \"en\", split='train', streaming=True)\n\n## spanish data language id 3\n# dataset_es = load_dataset(\"facebook/voxpopuli\", \"es\", split='train', streaming=True)\n\n## romanian data language id 6\n# dataset_ro = load_dataset(\"facebook/voxpopuli\", \"ro\", split='train', streaming=True)\n\n\n\nData Selection\nAs learned in the lectures, we will be adding only 50 samples from each of these datasets.\n\n# subset = []\n# for i, example in enumerate(dataset):\n#     if i &gt;= 50:\n#         break\n#     subset.append(example)\n\n# for i, example in enumerate(dataset_es):\n#     if i &gt;= 200:\n#         break\n#     subset.append(example)\n\n# for i, example in enumerate(dataset_ro):\n#     if i &gt;= 200:\n#         break\n#     subset.append(example)\n\nYou can check the the length of the dataset. It should be 150 in our case.\n\n# len(subset)\n\nTo see the first row of the dataset -\n\n# subset[0]\n\nNow, notice the change of language from 0 to 3 at 51st row.\n\n# subset[51]\n\nWe will using random shuffling, so as to remove the bias of particular language being on a particular row.\n\n# import random\n# random.shuffle(subset)\n\n\n\nExtracting the Features\nWe will be loading the Wave2Vec2 model from transformers library. This will help us to extract the features from the audio clips. Also, we are using the tqdm library to see the progress bar.\n\n# import torch\n# from transformers import Wav2Vec2Processor, Wav2Vec2Model\n# from tqdm import tqdm\n\n# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n# model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n# call_count=0\n# def extract_wav2vec_features(batch):\n#     global call_count\n#     call_count+=1\n#     print(call_count)\n#     inputs = processor(batch[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\", padding=True)\n#     with torch.no_grad():\n#         outputs = model(**inputs)\n#     return {\"features\": outputs.last_hidden_state.squeeze().mean(dim=0).numpy(), \"label\": batch[\"language\"]}\n\nUse the above defined function, to extract the features from 150 audio clips. Please note that this cell might take around 5 to 6 minutes to run, depending upon your CPU.\n\n# subset_with_features = [extract_wav2vec_features(example) for example in subset]\n\n\n\nSplitting the Dataset\nWe will be using our old sklearn library to split the dataset into train and test dataset. Further, train dataset will be split into train and validation dataset.\n\n# from sklearn.model_selection import train_test_split\n\n# train_data, test_data = train_test_split(subset_with_features, test_size=0.2)\n# train_data, val_data = train_test_split(train_data, test_size=0.1)\n\n# def prepare_data(data):\n#     features = [item[\"features\"] for item in data]\n#     labels = [item[\"label\"] for item in data]\n#     return torch.tensor(features), torch.tensor(labels)\n\n\n# train_features, train_labels = prepare_data(train_data)\n# val_features, val_labels = prepare_data(val_data)\n# test_features, test_labels = prepare_data(test_data)\n\n\n\nDefining the Model and its Parameters - Class Structure\nNow we will be using torch library to define our neural network.\n\nInitialization (init method): Takes two parameters - input_dim (the number of input channels) and num_classes (the number of output classes). Define the following layers:\n\nFirst Convolutional Layer (conv1):\nInput channels: input_dim Output channels: 128 Kernel size: 3 Padding: 1\nSecond Convolutional Layer (conv2):\nInput channels: 128 Output channels: 256 Kernel size: 3 Padding: 1\nFirst Fully Connected Layer (fc1):\nInput features: 256 * (input_length // 4) Output features: 128\nSecond Fully Connected Layer (fc2):\nInput features: 128 Output features: num_classes\n\nForward Pass (forward method): Defines the flow of data through the network.\n\nApply conv1, followed by ReLU activation and max pooling (kernel size 2)\nApply conv2, followed by ReLU activation and max pooling (kernel size 2)\nFlatten the output\nApply fc1 with ReLU activation\nApply fc2 to produce final output\n\n\nImplementation -\n\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torch.optim as optim\n\n# class CNNModel(nn.Module):\n#     def __init__(self, input_dim, num_classes):\n#         super(CNNModel, self).__init__()\n#         self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=128, kernel_size=3, padding=1)\n#         self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n#         self.fc1 = nn.Linear(256 * (input_length // 4), 128)  # Adjust the input dimension to the linear layer\n#         self.fc2 = nn.Linear(128, num_classes)\n\n#     def forward(self, x):\n#         x = F.relu(self.conv1(x))\n#         x = F.max_pool1d(x, kernel_size=2)\n#         x = F.relu(self.conv2(x))\n#         x = F.max_pool1d(x, kernel_size=2)\n#         x = x.view(x.size(0), -1)\n#         x = F.relu(self.fc1(x))\n#         x = self.fc2(x)\n#         return x\n\n\n\nModel Training\nOnce, we have defined our model. It’s time we train it and check the validation loss after each epoch.\n\n## Assuming train_features and val_features are already loaded\n## And they have the shape [batch_size, sequence_length]\n# input_length = train_features.shape[1]  # Add this line to get the sequence length\n\n## Assuming we have 3 languages in the dataset, and the original labels are 6, 0, and 3\n# label_mapping = {6: 0, 0: 1, 3: 2}\n\n## Apply the label mapping\n# train_labels = train_labels.apply_(lambda x: label_mapping[x])\n# val_labels = val_labels.apply_(lambda x: label_mapping[x])\n# test_labels = test_labels.apply_(lambda x: label_mapping[x])\n\n## Verify the mapping\n# unique_train_labels = torch.unique(train_labels)\n# unique_val_labels = torch.unique(val_labels)\n# unique_test_labels = torch.unique(test_labels)\n# print(f\"Mapped unique train labels: {unique_train_labels}\")\n# print(f\"Mapped unique validation labels: {unique_val_labels}\")\n# print(f\"Mapped unique test labels: {unique_test_labels}\")\n\n## Initialize the model\n# model = CNNModel(input_dim=1, num_classes=3)\n\n## Define loss function and optimizer\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n## Training loop\n# num_epochs = 20\n# for epoch in range(num_epochs):\n#     model.train()\n#     optimizer.zero_grad()\n#     outputs = model(train_features.unsqueeze(1).float())\n#     loss = criterion(outputs, train_labels)\n#     loss.backward()\n#     optimizer.step()\n\n## Validation\n#     model.eval()\n#     with torch.no_grad():\n#         val_outputs = model(val_features.unsqueeze(1).float())\n#         val_loss = criterion(val_outputs, val_labels)\n\n#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n\nWe can put a threshold after which if the loss and validation loss are below the threshold we can stop the training post that.\n\n\nTesting/Inference Step\nWe can now use the trained model to test on the unseen data and calculate it’s accuracy - of how the model is performing. We will be using the accuracy_score metric from sklearn library for this purpose.\n\n# import torch\n# from sklearn.metrics import accuracy_score\n\n## Ensure your model is in evaluation mode\n# model.eval()\n\n## Disable gradient computation\n# with torch.no_grad():\n#     # Forward pass to get predictions\n#     test_outputs = model(test_features.unsqueeze(1).float())\n\n\n## Get the predicted class labels\n#     _, predicted = torch.max(test_outputs, 1)\n\n## Calculate accuracy\n#     accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n\n## Print the test accuracy\n# print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n\n\n\nSuggestions for the Students\nThe students can try the following exercise to learn more about this experiment -\n\nIncease the dataset size from 50 to n.\nAdjust the number of epochs - go for training more epochs if the accuracy still improves.\nExperiment with different model architectures - different CNN configurations or even try RNNs. See what happens!\nYou can try to vary the feature extraction models - use different pre-trained models like HuBERT, Wav2Vec-U. Or you can experiment by changing the layers.\nTry different data augmentation techniques like time stretching, pitch shifting - they should be relevant on continuous time signals or waveforms.\nHyperparameter tuning is always present - play with learning rate, batch size, etc.\nYou can try to apply complex cross-validation methods like k-fold for more robust evaluation.",
    "crumbs": [
      "Week 5",
      "L3: Language Identification - Demo"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Hugging Face has proven to be an indispensable tool in the artificial intelligence (AI) domain. It has paved a long way, evolving from a niche-specific chatbot startup into a $4.5 billion open-source powerhouse and delivering access to machine learning, deep learning, and other associated domains. It was founded sometime around 2016. Initially, the company started as a chatbot app that used to target teenagers to provide emotional support via an AI BFF (Best Friend Forever). Soon enough, the founders realized that it had a lot of potential than just being used as a chatbot. They found the underlying principle of natural language processing (NLP) technology. Finally, open-sourced in 2018, they started building a collaborative environment for AI/ML development. This decision led them to become the major leader in the ML ecosystem. The Transformers library was released by Hugging Face (hereafter referred to as HF) in 2018. It was a toolkit based on Python Language that provided simplified access to pre-trained models like BERT and GPT.\n\n## Use this to download the Transformers library into your environment.\n\n## Uncomment the Code (line starting with a single #) and run it in your choice of environment. \n\n# !pip install transformers\n\nThis was huge. It implemented state-of-the-art NLP without having to procure all the extensive computational resources. This modular design was accepted widely and the documentation flourished. By 2020, 1000 open-source models were available. The open-source feature allowed the global community to participate as contributors. Thus, innovation accelerated with no bounds. Slowly, all kinds of NLP tasks like text classification, sentiment analysis, multilingual translation, etc. HF had models and datasets for all of them. Slowly in the 2020s, the platform decided to go beyond NLP. The related domains like Reinforcement Learning, and computer vision were introduced. At the same time, enterprise-level features were introduced, that included Autotraining, private cloud hosting, and better and informed security protocols. The demand for HF started growing and Fortune 500 companies like Intel, and Bloomberg, started aligning HF in commercialization.\nIn 2022, HF launched BLOOM, a multilingual large language model with 176 billion parameters, this made the community hit with a critical mass. BLOOM supported 46 languages and 13 programming languages which almost challenged the existing models like GPT-3 (trained on 175 billion parameters). The platform surpassed 1 million AI models in October 2024. The tools enhanced for image generation (Stable Diffusion), writing code (CodeGen), and audio processing. This milestone led to, HF being referred to as “GitHub of Machine Learning.” The major strategic partnership came to HF from AWS integrated into HF tools via Trainium chips. Soon, other collaborations from Meta and UNESCO launched a translator that supported 200 languages, even with some specific dialects. Investors from Google, NVIDIA, AMD, etc recognized the potential and the company achieved a $4.5 billion valuation.\nA few of the recent expansions show the HF’s ambition to lead the AI workspace -\n\nEuropean AI Accelerator was launched in June 2024 with Meta and Scaleway. This program acts as a mentor for the startups in Paris to integrate open models to reach a stage of commercial products.\nAI Ethics - Allowing BLOOM’s open weights helped to establish a standard model card and transparency in the dataset. It helped to promote reproducibility and advocated ethical AI.\nAcquisitions - In 2022, HF acquired Gradio, another powerful, open-source ML App builder. It helped many developers to streamline deployment pipelines.\n\nDelangue once stated, “Smaller, yet specialized models often outperform generic ones.” According to the philosophy, HF is leading the coming wave of artificial intelligence. It helps to make the future more helpful, and adaptable, empowering diverse applications using open tools and applicable everywhere ranging from healthcare diagnostics to climate modeling. Thus, I hope the incoming students of Deep Learning Practice, can understand the exemplification of HF by following the open-source principles to scale innovation at a global level, doing all, by maintaining ethical accountability.",
    "crumbs": [
      "Week 0",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#evolution-of-hugging-face",
    "href": "intro.html#evolution-of-hugging-face",
    "title": "Introduction",
    "section": "",
    "text": "Hugging Face has proven to be an indispensable tool in the artificial intelligence (AI) domain. It has paved a long way, evolving from a niche-specific chatbot startup into a $4.5 billion open-source powerhouse and delivering access to machine learning, deep learning, and other associated domains. It was founded sometime around 2016. Initially, the company started as a chatbot app that used to target teenagers to provide emotional support via an AI BFF (Best Friend Forever). Soon enough, the founders realized that it had a lot of potential than just being used as a chatbot. They found the underlying principle of natural language processing (NLP) technology. Finally, open-sourced in 2018, they started building a collaborative environment for AI/ML development. This decision led them to become the major leader in the ML ecosystem. The Transformers library was released by Hugging Face (hereafter referred to as HF) in 2018. It was a toolkit based on Python Language that provided simplified access to pre-trained models like BERT and GPT.\n\n## Use this to download the Transformers library into your environment.\n\n## Uncomment the Code (line starting with a single #) and run it in your choice of environment. \n\n# !pip install transformers\n\nThis was huge. It implemented state-of-the-art NLP without having to procure all the extensive computational resources. This modular design was accepted widely and the documentation flourished. By 2020, 1000 open-source models were available. The open-source feature allowed the global community to participate as contributors. Thus, innovation accelerated with no bounds. Slowly, all kinds of NLP tasks like text classification, sentiment analysis, multilingual translation, etc. HF had models and datasets for all of them. Slowly in the 2020s, the platform decided to go beyond NLP. The related domains like Reinforcement Learning, and computer vision were introduced. At the same time, enterprise-level features were introduced, that included Autotraining, private cloud hosting, and better and informed security protocols. The demand for HF started growing and Fortune 500 companies like Intel, and Bloomberg, started aligning HF in commercialization.\nIn 2022, HF launched BLOOM, a multilingual large language model with 176 billion parameters, this made the community hit with a critical mass. BLOOM supported 46 languages and 13 programming languages which almost challenged the existing models like GPT-3 (trained on 175 billion parameters). The platform surpassed 1 million AI models in October 2024. The tools enhanced for image generation (Stable Diffusion), writing code (CodeGen), and audio processing. This milestone led to, HF being referred to as “GitHub of Machine Learning.” The major strategic partnership came to HF from AWS integrated into HF tools via Trainium chips. Soon, other collaborations from Meta and UNESCO launched a translator that supported 200 languages, even with some specific dialects. Investors from Google, NVIDIA, AMD, etc recognized the potential and the company achieved a $4.5 billion valuation.\nA few of the recent expansions show the HF’s ambition to lead the AI workspace -\n\nEuropean AI Accelerator was launched in June 2024 with Meta and Scaleway. This program acts as a mentor for the startups in Paris to integrate open models to reach a stage of commercial products.\nAI Ethics - Allowing BLOOM’s open weights helped to establish a standard model card and transparency in the dataset. It helped to promote reproducibility and advocated ethical AI.\nAcquisitions - In 2022, HF acquired Gradio, another powerful, open-source ML App builder. It helped many developers to streamline deployment pipelines.\n\nDelangue once stated, “Smaller, yet specialized models often outperform generic ones.” According to the philosophy, HF is leading the coming wave of artificial intelligence. It helps to make the future more helpful, and adaptable, empowering diverse applications using open tools and applicable everywhere ranging from healthcare diagnostics to climate modeling. Thus, I hope the incoming students of Deep Learning Practice, can understand the exemplification of HF by following the open-source principles to scale innovation at a global level, doing all, by maintaining ethical accountability.",
    "crumbs": [
      "Week 0",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#usage-of-hugging-face-and-best-practices",
    "href": "intro.html#usage-of-hugging-face-and-best-practices",
    "title": "Introduction",
    "section": "Usage of Hugging Face and Best Practices",
    "text": "Usage of Hugging Face and Best Practices\nAs discussed, Hugging Face is called the GitHub of Machine Learning. It offers around 300,000+ pre-trained models, 50,000+ datasets, number of tools to build or deploy your AI applications. These customizable models and datasets can be trained and tested for various domains. There are several open-source libraries like transformers (specifically designed for Natural Language Processing - this will be highly used in our course work from Week 1 to 4), diffusers (designed for image generation), datasets (download datasets directly by specifying the URL and intended sub-parts). They collectively help us in working with modern AI architectures like BERT, GPT, and stable diffusion.\nKey Components -\n\nModels: Pre-trained architectures like BERT, GPT, and Diffusion for NLP, vision, and audio tasks. Ready to download and use. Please note some of the models are gated, they require special permissions from the owners. For eg. meta-llama/Llama-3.2-1B\nDatasets: Curated and clean data ready for training or evaluation. We will learn in the next module how to download and play with the datasets.\nSpaces: Host ML demos using Gradio or Streamlit. Not very relevant for this particular course, but highly recommended for real-life industrial applications.\nInference API: Serverless and seamless experience in model testing.\n\nSetup for Deployment\n1. VS Code\n\nInstall Hugging Face VS Code Extension\nConfigure API token:\n\n## In VS Code command palette (Ctrl+Shift+P):\n# Hugging Face Code: Set API Token\n\nUse code autocompletion with StarCoder/Codellama\n\n2. Google Colab\n\n## Install core libraries\n# !pip install transformers datasets huggingface_hub\n\n## Authenticate\n# from huggingface_hub import notebook_login\n# notebook_login()  # Follow token prompt\n\n3. CLI in Token Management\n\nCreate token at hf.co/settings/tokens\nTerminal login:\n\n# huggingface-cli login\n## Enter token when prompted\n\nUse in Python:\n\n\n# from huggingface_hub import login\n# login(token=\"hf_yourtoken\")  # For script authentication\n\n\nBest Practices\n\nModel Selection\n\n\nFilter by task/library/dataset on Model Hub\nCheck model cards for license/compute requirements\n\n\nResource Management\n\n\nUse fp16 precision: model.half()\nLeverage model parallelism for large models\n\n\nReproducibility\n\n\n# from transformers import set_seed\n# set_seed(42)  # Across all ML libraries\n\n\nProduction Deployment\n\n\nConvert models to ONNX/TensorRT\nUse Inference Endpoints for scalable serving\n\n\nConvention used in our study guide -\n\n\n# (Single hash): Executable code (uncomment to use)\n## (Double hash): Explanatory comments",
    "crumbs": [
      "Week 0",
      "Introduction"
    ]
  }
]