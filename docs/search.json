[
  {
    "objectID": "sneak.html",
    "href": "sneak.html",
    "title": "Sneak Peak into Important Libraries",
    "section": "",
    "text": "In this section, we will discuss the important libraries that we will be using in the Deep Learning Course.\n\nHugging Face Datasets\n\nTo install -\n\n# !pip install datasets\n\nHugging Face Datasets is a powerful library provided by Hugging Face that simplifies loading and processing datasets for various machine learning and deep learning tasks. Over the years, it has become a unified interface for thousands of datasets available on the hub. It provides datasets for various domains starting from audio, computer vision, and natural language processing. The library allows the users to load the datasets with a single piece of code and offers advanced processing algorithms to prepare and customize the data for training in deep learning models. Further, it allows to implement of the various performance metrics that are needed for transformer-based model evaluations.\nKey features -\n\n\nEasy access to a vast collection of curated datasets from across the world.\n\n\nEfficient data loading and further processing capabilities.\n\n\nEasy integration with other related libraries.\n\n\nBuilt-in utilities for data preparation, splitting, shuffling, and mapping.\n\n\nFlexibility to fine-tune models on customized needs and datasets.\n\n\n\nHugging Face Transformers\n\nTo install -\n\n# !pip install transformers\n\nThe Hugging Face Transformers is another important and comprehensive library that provides state-of-the-art pre-trained models for various natural language processing tasks. It offers all types of models including - BERT, GPT, and T5, which can be loaded and fine-tuned for customizable tasks. The library supports other multiple deep learning frameworks such as PyTorch, TensorFlow, and JAX. Further, it makes it versatile for different development environments.\nKey features -\n\n\nAccess to a vast collection of pre-trained models.\n\n\nSupport most of the deep learning frameworks.\n\n\nEasy to use APIs for inference and future fine-tuning.\n\n\nExtensive community support is well maintained by tech leaders globally.\n\n\n\nPyTorch\n\nTo install -\n\n# !pip install torch\n\nPyTorch is commonly known as Torch. It is one of the most powerful open-source machine-learning libraries developed by Facebook. It is known for its dynamic computational graphs - that allows for more flexible and intuitive model development. It is quite popular among researchers in the field of deep learning due to its ease of use and powerful capabilities.\nKey features -\n\n\nDynamic computational graphs for flexible model development.\n\n\nEfficient GPU (like T4, A100, etc) acceleration for faster training and inference.\n\n\nFull collection of tools and libraries for various ML and DL tasks.\n\n\nRegular updates from the community keep everything up-to-date.\n\n\n\nPyannote\n\nTo install -\n\n# !pip install pyannote\n\nPyannote is an open-source Python toolkit. It is specifically designed for speaker diarization tasks. We will be using this library in Week 6 (the speech part). It helps to identify and cluster the fixed number of speakers from the given audio, this process in a nutshell is known as the speaker diarization. It uses the PyTorch deep learning framework. It provides trained models and pipelines, especially efficient in audio analysis. Further, it is also useful in tasks such as speaker segmentation, speaker clustering, and overlapped speech detection.\nKey features -\n\n\nProvides pre-trained models for speaker diarization tasks.\n\n\nHelps to assemble or create pipelines for audio processing.\n\n\nProvides GPU acceleration support for faster processing.\n\n\nFlexibility to fine-tune models on customized needs and datasets.\n\n\n\nWhisper\n\nTo install -\n\n# !pip install git+https://github.com/openai/whisper.git\n\n## Also, download FFmpeg for handling various audio formats with ease.\n\n# !pip install ffmpeg\n\nWhisper is another very powerful pre-trained model developed by OpenAI. We will be using this in Weeks 6 and 7. It helps in automatic speech recognition (commonly referred to as ASR) and speech translation. It has been trained on around 680,000 hours of labeled data (mostly in western languages). The vast amount of labeled data helps to generalize well across various datasets and domains, sometimes even without further fine-tuning. Whisper is available to use in different model sizes ranging from tiny to large. This helps to accommodate different computational requirements and varied use cases.\nKey features -\n\n\nRobust performance in various languages and certain accents.\n\n\nSupports speech recognition and translation tasks.\n\n\nAvailability of different model sizes making it usable for varied purposes.\n\n\nAvailable as open-source, thereby, creating a strong community.",
    "crumbs": [
      "Week 0",
      "Sneak Peak"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DLP Study Guide",
    "section": "",
    "text": "Table of Contents\n\nWeek 0\n\nIntroduction\nSneak Peak",
    "crumbs": [
      "Index"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Hugging Face has proven to be an indispensable tool in the artificial intelligence (AI) domain. It has paved a long way, evolving from a niche-specific chatbot startup into a $4.5 billion open-source powerhouse and delivering access to machine learning, deep learning, and other associated domains. It was founded sometime around 2016. Initially, the company started as a chatbot app that used to target teenagers to provide emotional support via an AI BFF (Best Friend Forever). Soon enough, the founders realized that it had a lot of potential than just being used as a chatbot. They found the underlying principle of natural language processing (NLP) technology. Finally, open-sourced in 2018, they started building a collaborative environment for AI/ML development. This decision led them to become the major leader in the ML ecosystem. The Transformers library was released by Hugging Face (hereafter referred to as HF) in 2018. It was a toolkit based on Python Language that provided simplified access to pre-trained models like BERT and GPT.\n\n## Use this to download the Transformers library into your environment.\n\n## Uncomment the Code (line starting with a single #) and run it in your choice of environment. \n\n# !pip install transformers\n\nThis was huge. It implemented state-of-the-art NLP without having to procure all the extensive computational resources. This modular design was accepted widely and the documentation flourished. By 2020, 1000 open-source models were available. The open-source feature allowed the global community to participate as contributors. Thus, innovation accelerated with no bounds. Slowly, all kinds of NLP tasks like text classification, sentiment analysis, multilingual translation, etc. HF had models and datasets for all of them. Slowly in the 2020s, the platform decided to go beyond NLP. The related domains like Reinforcement Learning, and computer vision were introduced. At the same time, enterprise-level features were introduced, that included Autotraining, private cloud hosting, and better and informed security protocols. The demand for HF started growing and Fortune 500 companies like Intel, and Bloomberg, started aligning HF in commercialization.\nIn 2022, HF launched BLOOM, a multilingual large language model with 176 billion parameters, this made the community hit with a critical mass. BLOOM supported 46 languages and 13 programming languages which almost challenged the existing models like GPT-3 (trained on 175 billion parameters). The platform surpassed 1 million AI models in October 2024. The tools enhanced for image generation (Stable Diffusion), writing code (CodeGen), and audio processing. This milestone led to, HF being referred to as “GitHub of Machine Learning.” The major strategic partnership came to HF from AWS integrated into HF tools via Trainium chips. Soon, other collaborations from Meta and UNESCO launched a translator that supported 200 languages, even with some specific dialects. Investors from Google, NVIDIA, AMD, etc recognized the potential and the company achieved a $4.5 billion valuation.\nA few of the recent expansions show the HF’s ambition to lead the AI workspace -\n\nEuropean AI Accelerator was launched in June 2024 with Meta and Scaleway. This program acts as a mentor for the startups in Paris to integrate open models to reach a stage of commercial products.\nAI Ethics - Allowing BLOOM’s open weights helped to establish a standard model card and transparency in the dataset. It helped to promote reproducibility and advocated ethical AI.\nAcquisitions - In 2022, HF acquired Gradio, another powerful, open-source ML App builder. It helped many developers to streamline deployment pipelines.\n\nDelangue once stated, “Smaller, yet specialized models often outperform generic ones.” According to the philosophy, HF is leading the coming wave of artificial intelligence. It helps to make the future more helpful, and adaptable, empowering diverse applications using open tools and applicable everywhere ranging from healthcare diagnostics to climate modeling. Thus, I hope the incoming students of Deep Learning Practice, can understand the exemplification of HF by following the open-source principles to scale innovation at a global level, doing all, by maintaining ethical accountability.",
    "crumbs": [
      "Week 0",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#evolution-of-hugging-face",
    "href": "intro.html#evolution-of-hugging-face",
    "title": "Introduction",
    "section": "",
    "text": "Hugging Face has proven to be an indispensable tool in the artificial intelligence (AI) domain. It has paved a long way, evolving from a niche-specific chatbot startup into a $4.5 billion open-source powerhouse and delivering access to machine learning, deep learning, and other associated domains. It was founded sometime around 2016. Initially, the company started as a chatbot app that used to target teenagers to provide emotional support via an AI BFF (Best Friend Forever). Soon enough, the founders realized that it had a lot of potential than just being used as a chatbot. They found the underlying principle of natural language processing (NLP) technology. Finally, open-sourced in 2018, they started building a collaborative environment for AI/ML development. This decision led them to become the major leader in the ML ecosystem. The Transformers library was released by Hugging Face (hereafter referred to as HF) in 2018. It was a toolkit based on Python Language that provided simplified access to pre-trained models like BERT and GPT.\n\n## Use this to download the Transformers library into your environment.\n\n## Uncomment the Code (line starting with a single #) and run it in your choice of environment. \n\n# !pip install transformers\n\nThis was huge. It implemented state-of-the-art NLP without having to procure all the extensive computational resources. This modular design was accepted widely and the documentation flourished. By 2020, 1000 open-source models were available. The open-source feature allowed the global community to participate as contributors. Thus, innovation accelerated with no bounds. Slowly, all kinds of NLP tasks like text classification, sentiment analysis, multilingual translation, etc. HF had models and datasets for all of them. Slowly in the 2020s, the platform decided to go beyond NLP. The related domains like Reinforcement Learning, and computer vision were introduced. At the same time, enterprise-level features were introduced, that included Autotraining, private cloud hosting, and better and informed security protocols. The demand for HF started growing and Fortune 500 companies like Intel, and Bloomberg, started aligning HF in commercialization.\nIn 2022, HF launched BLOOM, a multilingual large language model with 176 billion parameters, this made the community hit with a critical mass. BLOOM supported 46 languages and 13 programming languages which almost challenged the existing models like GPT-3 (trained on 175 billion parameters). The platform surpassed 1 million AI models in October 2024. The tools enhanced for image generation (Stable Diffusion), writing code (CodeGen), and audio processing. This milestone led to, HF being referred to as “GitHub of Machine Learning.” The major strategic partnership came to HF from AWS integrated into HF tools via Trainium chips. Soon, other collaborations from Meta and UNESCO launched a translator that supported 200 languages, even with some specific dialects. Investors from Google, NVIDIA, AMD, etc recognized the potential and the company achieved a $4.5 billion valuation.\nA few of the recent expansions show the HF’s ambition to lead the AI workspace -\n\nEuropean AI Accelerator was launched in June 2024 with Meta and Scaleway. This program acts as a mentor for the startups in Paris to integrate open models to reach a stage of commercial products.\nAI Ethics - Allowing BLOOM’s open weights helped to establish a standard model card and transparency in the dataset. It helped to promote reproducibility and advocated ethical AI.\nAcquisitions - In 2022, HF acquired Gradio, another powerful, open-source ML App builder. It helped many developers to streamline deployment pipelines.\n\nDelangue once stated, “Smaller, yet specialized models often outperform generic ones.” According to the philosophy, HF is leading the coming wave of artificial intelligence. It helps to make the future more helpful, and adaptable, empowering diverse applications using open tools and applicable everywhere ranging from healthcare diagnostics to climate modeling. Thus, I hope the incoming students of Deep Learning Practice, can understand the exemplification of HF by following the open-source principles to scale innovation at a global level, doing all, by maintaining ethical accountability.",
    "crumbs": [
      "Week 0",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#usage-of-hugging-face-and-best-practices",
    "href": "intro.html#usage-of-hugging-face-and-best-practices",
    "title": "Introduction",
    "section": "Usage of Hugging Face and Best Practices",
    "text": "Usage of Hugging Face and Best Practices\nAs discussed, Hugging Face is called the GitHub of Machine Learning. It offers around 300,000+ pre-trained models, 50,000+ datasets, number of tools to build or deploy your AI applications. These customizable models and datasets can be trained and tested for various domains. There are several open-source libraries like transformers (specifically designed for Natural Language Processing - this will be highly used in our course work from Week 1 to 4), diffusers (designed for image generation), datasets (download datasets directly by specifying the URL and intended sub-parts). They collectively help us in working with modern AI architectures like BERT, GPT, and stable diffusion.\nKey Components -\n\nModels: Pre-trained architectures like BERT, GPT, and Diffusion for NLP, vision, and audio tasks. Ready to download and use. Please note some of the models are gated, they require special permissions from the owners. For eg. meta-llama/Llama-3.2-1B\nDatasets: Curated and clean data ready for training or evaluation. We will learn in the next module how to download and play with the datasets.\nSpaces: Host ML demos using Gradio or Streamlit. Not very relevant for this particular course, but highly recommended for real-life industrial applications.\nInference API: Serverless and seamless experience in model testing.\n\nSetup for Deployment\n1. VS Code\n\nInstall Hugging Face VS Code Extension\nConfigure API token:\n\n## In VS Code command palette (Ctrl+Shift+P):\n# Hugging Face Code: Set API Token\n\nUse code autocompletion with StarCoder/Codellama\n\n2. Google Colab\n\n## Install core libraries\n# !pip install transformers datasets huggingface_hub\n\n## Authenticate\n# from huggingface_hub import notebook_login\n# notebook_login()  # Follow token prompt\n\n3. CLI in Token Management\n\nCreate token at hf.co/settings/tokens\nTerminal login:\n\n# huggingface-cli login\n## Enter token when prompted\n\nUse in Python:\n\n\n# from huggingface_hub import login\n# login(token=\"hf_yourtoken\")  # For script authentication\n\n\nBest Practices\n\nModel Selection\n\n\nFilter by task/library/dataset on Model Hub\nCheck model cards for license/compute requirements\n\n\nResource Management\n\n\nUse fp16 precision: model.half()\nLeverage model parallelism for large models\n\n\nReproducibility\n\n\n# from transformers import set_seed\n# set_seed(42)  # Across all ML libraries\n\n\nProduction Deployment\n\n\nConvert models to ONNX/TensorRT\nUse Inference Endpoints for scalable serving\n\n\nConvention used in our study guide -\n\n\n# (Single hash): Executable code (uncomment to use)\n## (Double hash): Explanatory comments",
    "crumbs": [
      "Week 0",
      "Introduction"
    ]
  }
]